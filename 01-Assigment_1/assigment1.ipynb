{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c53ox5Nrb11E"
      },
      "source": [
        "# Language Identification"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacremoses"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3WpTdk0c5ps",
        "outputId": "3b2d0173-3c38-42c3-b7a1-66629fe32354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacremoses) (2024.11.6)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from sacremoses) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from sacremoses) (1.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sacremoses) (4.67.1)\n",
            "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/897.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m890.9/897.5 kB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOehGQAlb11I"
      },
      "source": [
        "## Tokenize data in the proper language\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xR736JXb11I"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "  \"\"\" Return the text in lower case, without strange characters and multiples spaces\"\"\"\n",
        "  text = text.lower()\n",
        "  text = re.sub(r'[^\\w\\s]', '', text)\n",
        "  text = re.sub(r'\\s+', ' ', text).strip()\n",
        "  return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "TdhQzy-Nb11J",
        "outputId": "7595682d-1b14-4470-d767-d26a17ad3939"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "The directory '/content/data' does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-ac53e90e5438>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mdirectory_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/data\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mtokenized_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-ac53e90e5438>\u001b[0m in \u001b[0;36mtokenize_files\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Check if the directory exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The directory '{directory}' does not exist.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Iterate over .txt files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: The directory '/content/data' does not exist."
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "from sacremoses import MosesTokenizer\n",
        "\n",
        "def tokenize_files(directory):\n",
        "    \"\"\"\n",
        "    Tokenizes the content of all .txt files in a given directory\n",
        "    and returns a dictionary where each filename is associated with its list of tokens.\n",
        "    The file name must be a language.\n",
        "    \"\"\"\n",
        "    tokenized_data = {}\n",
        "\n",
        "    # Check if the directory exists\n",
        "    if not os.path.isdir(directory):\n",
        "        raise FileNotFoundError(f\"The directory '{directory}' does not exist.\")\n",
        "\n",
        "    # Iterate over .txt files\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            file_path = os.path.join(directory, filename)\n",
        "\n",
        "            # Determine the language from the filename\n",
        "            language = filename.replace(\".txt\", \"\").lower()\n",
        "            try:\n",
        "                tokenizer = MosesTokenizer(lang=language)  # Initialize tokenizer based on language\n",
        "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                    content = clean_text(f.read())\n",
        "                    tokens = tokenizer.tokenize(content)  # Tokenization\n",
        "                    tokenized_data[language] = tokens  # Store tokens in dictionary\n",
        "\n",
        "                print(f\"Tokenization completed ({language}): {filename}\")\n",
        "                print(f\"Number of tokens in {language}: {len(tokens)} tokens\")\n",
        "                print(f\"Size of the data in {language} : {sys.getsizeof(tokens)} bytes\")\n",
        "                print(\"\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error with {filename} ({language}): {e}\")\n",
        "\n",
        "    return tokenized_data\n",
        "\n",
        "directory_path = \"/content/data\"\n",
        "tokenized_results = tokenize_files(directory_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjWeUmXUb11L"
      },
      "source": [
        "## Split Data\n",
        "We split the data into the training set, heldout and test sets. We will use 80% of the data for training.\n",
        "The"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "cqOuH1vrb11L",
        "outputId": "468e0558-ed8c-4a0e-eee6-22625711484d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tokenized_results' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-e1fa5ed88d26>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheldout_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0msets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_sets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenized_results' is not defined"
          ]
        }
      ],
      "source": [
        "def create_sets(tokenized_data):\n",
        "  \"\"\" Create training, held Out and test data set with tokenized_data\n",
        "  and returns all sets.\"\"\"\n",
        "\n",
        "  training_data={}\n",
        "  test_data={}\n",
        "  heldout_data={}\n",
        "\n",
        "  for language, tokens in tokenized_data.items():\n",
        "      split_training_point=int(len(tokens)*0.8)\n",
        "      split_test_point=int(len(tokens)*0.1+split_training_point)\n",
        "      training_data[language]=tokens[:split_training_point]\n",
        "      test_data[language]=tokens[split_training_point:split_test_point]\n",
        "      heldout_data[language]=tokens[split_test_point:]\n",
        "\n",
        "  return training_data,test_data,heldout_data\n",
        "\n",
        "sets=create_sets(tokenized_results)\n",
        "training_data=sets[0]\n",
        "test_data=sets[1]\n",
        "heldout_data=sets[2]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_oov_percentage(training_data, data):\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    oov_percentages = {}\n",
        "\n",
        "    for language in _data:\n",
        "        training_vocab = set(training_data.get(language))\n",
        "        tokens = data.get(language)\n",
        "\n",
        "        oov_tokens = [token for token in tokens if token not in training_vocab]\n",
        "        oov_percentage = (len(oov_tokens) / len(tokens)) * 100 if tokens else 0\n",
        "\n",
        "        oov_percentages[language] = round(oov_percentage, 2)\n",
        "\n",
        "    return oov_percentages\n",
        "\n",
        "    print(calculate_oov_percentage(training_data, heldout_data))\n",
        "    print(calculate_oov_percentage(training_data, test_data))"
      ],
      "metadata": {
        "id": "IAKvr1AmhzU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UG44lWLXb11L"
      },
      "source": [
        "## Probabilities of n-grams\n",
        "\n",
        "First, we create n-grams (characters or tokens).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuPH5NKYb11M"
      },
      "outputs": [],
      "source": [
        "def extract_charac_ngrams(tokens, n):\n",
        "  \"\"\"Extract characters n-grams from a given list of tokens\n",
        "  and returns a list of n-grams\"\"\"\n",
        "  return [token[i:i+n] for token in tokens for i in range(len(token) - n + 1)]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJw5kXhAb11M"
      },
      "outputs": [],
      "source": [
        "def extract_tokens_ngrams(tokens, n):\n",
        "  \"\"\"Extract tokens n-grams from a given list of tokens\n",
        "  and returns a list of n-grams\"\"\"\n",
        "  ngrams_list = []\n",
        "  for i in range(len(tokens) - n + 1):\n",
        "      ngram = tuple(tokens[i:i + n])\n",
        "      ngrams_list.append(ngram)\n",
        "\n",
        "  return ngrams_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxJj_eTGb11M"
      },
      "source": [
        "Now, we compute the probabilities and return a dictionary of ``` ngrams ```.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLm-O9Svb11M"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def compute_ngram_probabilities(extract_ngrams,tokens, n):\n",
        "    \"\"\"\n",
        "    Compute n-gram probabilities for a given list of tokens\n",
        "    and returns a dictionary of probabilities, n-gram counts for each and the total number of n-gram.\n",
        "    \"\"\"\n",
        "    ngrams = extract_ngrams(tokens, n)\n",
        "    ngram_counts = Counter(ngrams)\n",
        "    total_ngrams = sum(ngram_counts.values())\n",
        "    if n == 1:\n",
        "\n",
        "        probabilities = {ngram: count / total_ngrams for ngram, count in ngram_counts.items()}\n",
        "    else:\n",
        "        n_minus_1_grams = extract_ngrams(tokens, n - 1)\n",
        "        n_minus_1_counts = Counter(n_minus_1_grams)\n",
        "        probabilities = {}\n",
        "        for ngram, count in ngram_counts.items():\n",
        "            n_minus_1_ngram = ngram[:-1]\n",
        "            if n_minus_1_ngram in n_minus_1_counts:\n",
        "                probabilities[ngram] = count / n_minus_1_counts[n_minus_1_ngram]  # P(w_n | w_{n-2}, w_{n-1})\n",
        "            else:\n",
        "                probabilities[ngram] = 0\n",
        "\n",
        "    return probabilities, ngram_counts, total_ngrams\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7ER2Zknb11N"
      },
      "outputs": [],
      "source": [
        "def get_most_common(ngram_counts, total_ngrams, top_n=5):\n",
        "    \"\"\"Return the top 5 most common  n-grams along with their counts and relative frequencies.\"\"\"\n",
        "    most_common = [(ngram, count, count / total_ngrams) for ngram, count in ngram_counts.most_common(top_n)]\n",
        "    return most_common\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1Ry82oub11N"
      },
      "outputs": [],
      "source": [
        "def get_least_common(ngram_counts, total_ngrams, top_n=5):\n",
        "    \"\"\"Return the top 5 least common n-grams along with their counts and relative frequencies.\"\"\"\n",
        "    least_common = [(ngram, count, count / total_ngrams) for ngram, count in ngram_counts.most_common()[-top_n:]]\n",
        "    return least_common\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRUVnxQ2b11N",
        "outputId": "3a3d5165-425b-4584-8e6a-fa16ccacb8a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Language: english\n",
            "Most Common: [('the', 21738, 0.02821563191177845), ('and', 12975, 0.016841375658079187), ('ing', 9499, 0.012329574364246181), ('her', 9441, 0.012254291143578082), ('hat', 5583, 0.0072466589825862125)]\n",
            "Least Common: [('ieg', 1, 1.2979865632430974e-06), ('870', 1, 1.2979865632430974e-06), ('shh', 1, 1.2979865632430974e-06), ('zyb', 1, 1.2979865632430974e-06), ('ghh', 1, 1.2979865632430974e-06)]\n",
            "-\n",
            "Data Language: french\n",
            "Most Common: [('ent', 5877, 0.011949469420785204), ('que', 5189, 0.01055058649386667), ('les', 4208, 0.008555958366966843), ('ait', 3614, 0.0073482018864586916), ('ant', 3558, 0.007234339322639741)]\n",
            "Least Common: [('wys', 1, 2.0332600681955427e-06), ('iqû', 1, 2.0332600681955427e-06), ('qûr', 1, 2.0332600681955427e-06), ('tga', 1, 2.0332600681955427e-06), ('fûm', 1, 2.0332600681955427e-06)]\n",
            "-\n",
            "Data Language: czech\n",
            "Most Common: [('sta', 2408, 0.0051783275808307255), ('byl', 2400, 0.005161123834715009), ('ost', 2266, 0.004872961087276755), ('pro', 2151, 0.004625657236863327), ('jak', 2141, 0.004604152554218681)]\n",
            "Least Common: [('zův', 1, 2.150468264464587e-06), ('ízo', 1, 2.150468264464587e-06), ('ízy', 1, 2.150468264464587e-06), ('ízá', 1, 2.150468264464587e-06), ('ůmě', 1, 2.150468264464587e-06)]\n",
            "-\n"
          ]
        }
      ],
      "source": [
        "def process_tokenized_data(tokenized_data):\n",
        "    \"\"\"\n",
        "    Process tokenized data and compute unigram, bigram, and trigram probabilities\n",
        "    and returns a dictionary with languages (filenames) as keys and n-gram probabilities, most and least common trigrams as values.\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "\n",
        "    for language, tokens in tokenized_data.items():\n",
        "        unigram_probs, unigram_counts, total_unigrams = compute_ngram_probabilities(extract_charac_ngrams,tokens, 1)\n",
        "        bigram_probs, bigram_counts, total_bigrams = compute_ngram_probabilities(extract_charac_ngrams,tokens, 2)\n",
        "        trigram_probs, trigram_counts, total_trigrams = compute_ngram_probabilities(extract_charac_ngrams,tokens, 3)\n",
        "\n",
        "        results[language] = {\n",
        "            'unigram': unigram_probs,\n",
        "            'bigram': bigram_probs,\n",
        "            'trigram': trigram_probs,\n",
        "            'most_common_trigrams': get_most_common(trigram_counts, total_trigrams),\n",
        "            'least_common_trigrams': get_least_common(trigram_counts, total_trigrams),\n",
        "        }\n",
        "\n",
        "    return results\n",
        "\n",
        "ngram_results = process_tokenized_data(training_data)\n",
        "\n",
        "\n",
        "for filename, ngrams in ngram_results.items():\n",
        "    print(f\"Data Language: {filename}\")\n",
        "    print(f\"Most Common: {ngrams['most_common_trigrams']}\")\n",
        "    print(f\"Least Common: {list(ngrams['least_common_trigrams'])}\")\n",
        "    print(\"-\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmxhdJ2Vb11O"
      },
      "source": [
        "## Add less than one and EM algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PILLdhlmb11O"
      },
      "outputs": [],
      "source": [
        "def compute_smoothed_ngram_probabilities(extract_ngrams,train_tokens, n, lambda_value):\n",
        "    \"\"\"Compute P'_{λ}(w | h) with smoothing add-less-than-one\n",
        "    and returns a dictionnary with n-gram as key and probabilities as values.\"\"\"\n",
        "    ngrams = extract_ngrams(train_tokens, n)\n",
        "    n_minus_1_grams = extract_ngrams(train_tokens, n - 1)\n",
        "    ngram_counts = Counter(ngrams)\n",
        "    n_minus_1_counts = Counter(n_minus_1_grams)\n",
        "    V = len(set(ngrams))\n",
        "\n",
        "    probabilities = {}\n",
        "    for ngram, count_h_w in ngram_counts.items():\n",
        "      count_h = n_minus_1_counts[ngram[:-1]]\n",
        "      probabilities[ngram] = (count_h_w + lambda_value) / (count_h + lambda_value * V)\n",
        "\n",
        "    return probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQiE2y_Xb11O"
      },
      "outputs": [],
      "source": [
        "def compute_interpolated_probabilities(extract_ngrams,train_tokens, lambdas, lambda_value):\n",
        "  \"\"\"compute P'_{λ}(w | h) with add-less-than-one and interpolated\n",
        "  and returns a dictionnary with n-gram as key and probabilities as values. \"\"\"\n",
        "  trigram_probs = compute_smoothed_ngram_probabilities(extract_ngrams,train_tokens, 3, lambda_value)\n",
        "  bigram_probs = compute_smoothed_ngram_probabilities(extract_ngrams,train_tokens, 2, lambda_value)\n",
        "  unigram_probs = compute_smoothed_ngram_probabilities(extract_ngrams,train_tokens, 1, lambda_value)\n",
        "  V = len(set(trigram_probs))\n",
        "\n",
        "  probabilities = {}\n",
        "  for trigram in trigram_probs:\n",
        "      w_i = trigram[-1]\n",
        "      w_i_1 = trigram[-2] if len(trigram) > 1 else \"\"\n",
        "      w_i_2 = trigram[-3] if len(trigram) > 2 else \"\"\n",
        "\n",
        "      p3 = trigram_probs.get((w_i_2,w_i_1, w_i), 0)\n",
        "      p2 = bigram_probs.get((w_i_1, w_i), 0)\n",
        "      p1 = unigram_probs.get((w_i,), 0)\n",
        "\n",
        "      probabilities[trigram] = lambdas[3] * p3 + lambdas[2] * p2 + lambdas[1] * p1 + lambdas[0] * (1/V)\n",
        "\n",
        "  return probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3vwB8jhb11O"
      },
      "outputs": [],
      "source": [
        "def optimize_lambdas_with_em(extract_ngrams,train_tokens, heldout_tokens, lambda_value, epsilon=0.03, max_iter=1000):\n",
        "    \"\"\"Optimize lambdas with EM algorithm and using add-less-than-one for smoothing\n",
        "    and returns an array of lambdas (float).\"\"\"\n",
        "    lambdas = [0.25, 0.25, 0.25, 0.25]\n",
        "\n",
        "    for iter in range(max_iter):\n",
        "        trigram_probs = compute_interpolated_probabilities(extract_ngrams,train_tokens, lambdas, lambda_value)\n",
        "        trigrams = extract_ngrams(heldout_tokens, 3)\n",
        "        expected_counts = [0, 0, 0, 0]\n",
        "\n",
        "        for trigram in trigrams:\n",
        "            p_lambda = sum(lambdas[i] * trigram_probs.get(trigram, 1e-10) for i in range(4))\n",
        "            for i in range(4):\n",
        "                expected_counts[i] += (lambdas[i] * trigram_probs.get(trigram, 1e-10)) / p_lambda\n",
        "        if iter % 10 == 0 :\n",
        "            print(f\"Iteration {iter+1}, Expected Counts: {expected_counts}\")\n",
        "\n",
        "        new_lambdas = [ec / sum(expected_counts) for ec in expected_counts]\n",
        "\n",
        "        print(f\"New Lambdas: {new_lambdas}\")\n",
        "\n",
        "        if all(abs(new_lambdas[i] - lambdas[i]) < epsilon for i in range(4)):\n",
        "            print(f\"Convergence atteinte après {iter+1} itérations.\")\n",
        "            break\n",
        "        lambdas = new_lambdas\n",
        "\n",
        "    return lambdas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YURvgqp4b11P"
      },
      "outputs": [],
      "source": [
        "\n",
        "lambda_value = 0.2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPaFe_Lnb11P"
      },
      "source": [
        "## Probabilities of our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ms_aZYNAb11P",
        "outputId": "ec374ef1-bbb4-46c0-c1e3-2ccb33a60d18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, Expected Counts: [10055.25, 10055.25, 10055.25, 10055.25]\n",
            "New Lambdas: [0.25, 0.25, 0.25, 0.25]\n",
            "Convergence atteinte après 1 itérations.\n",
            "Iteration 1, Expected Counts: [5864.5, 5864.5, 5864.5, 5864.5]\n",
            "New Lambdas: [0.25, 0.25, 0.25, 0.25]\n",
            "Convergence atteinte après 1 itérations.\n",
            "Iteration 1, Expected Counts: [5055.25, 5055.25, 5055.25, 5055.25]\n",
            "New Lambdas: [0.25, 0.25, 0.25, 0.25]\n",
            "Convergence atteinte après 1 itérations.\n",
            "Iteration 1, Expected Counts: [22774.25, 22774.25, 22774.25, 22774.25]\n",
            "New Lambdas: [0.25, 0.25, 0.25, 0.25]\n",
            "Convergence atteinte après 1 itérations.\n",
            "Iteration 1, Expected Counts: [15113.75, 15113.75, 15113.75, 15113.75]\n",
            "New Lambdas: [0.25, 0.25, 0.25, 0.25]\n",
            "Convergence atteinte après 1 itérations.\n",
            "Iteration 1, Expected Counts: [14331.25, 14331.25, 14331.25, 14331.25]\n",
            "New Lambdas: [0.25, 0.25, 0.25, 0.25]\n",
            "Convergence atteinte après 1 itérations.\n"
          ]
        }
      ],
      "source": [
        "training_trigram_probs = {}\n",
        "for language, tokens in training_data.items():\n",
        "    lambdas = optimize_lambdas_with_em(extract_tokens_ngrams, tokens, heldout_data.get(language), lambda_value)\n",
        "    trigram_probs = compute_interpolated_probabilities(extract_tokens_ngrams, tokens, lambdas, lambda_value)\n",
        "    training_trigram_probs[language] = trigram_probs\n",
        "\n",
        "\n",
        "training_trigram_charac_probs = {}\n",
        "for language, tokens in training_data.items():\n",
        "    lambdas = optimize_lambdas_with_em(extract_charac_ngrams, tokens, heldout_data.get(language), lambda_value)\n",
        "    trigram_probs = compute_interpolated_probabilities(extract_charac_ngrams, tokens, lambdas, lambda_value)\n",
        "    training_trigram_charac_probs[language] = trigram_probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7Scd_Gpb11P"
      },
      "source": [
        "## Compute Cross Entropy of trigram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HeOBCS06b11P"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_cross_entropy(trigrams, trigram_probs):\n",
        "  \"\"\" Compute the cross entropy\n",
        "  and returns it as a float.\"\"\"\n",
        "  H = len(trigrams)\n",
        "  entropy = -sum(np.log2(trigram_probs.get(trigram,1e-10)) for trigram in trigrams) / H\n",
        "  return entropy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMuxIYGqb11Q",
        "outputId": "a101e730-47cf-4122-cf0a-3c7445293315"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Language Training : english, Language test : english\n",
            " Proba Cross-Entropy :27.749919845556107\n",
            "\n",
            "\n",
            "Language Training : english, Language test : french\n",
            " Proba Cross-Entropy :33.03314596554471\n",
            "\n",
            "\n",
            "Language Training : english, Language test : czech\n",
            " Proba Cross-Entropy :33.2192809488651\n",
            "\n",
            "\n",
            "Language Training : french, Language test : english\n",
            " Proba Cross-Entropy :33.21781047076802\n",
            "\n",
            "\n",
            "Language Training : french, Language test : french\n",
            " Proba Cross-Entropy :29.52630606214211\n",
            "\n",
            "\n",
            "Language Training : french, Language test : czech\n",
            " Proba Cross-Entropy :33.2192809488651\n",
            "\n",
            "\n",
            "Language Training : czech, Language test : english\n",
            " Proba Cross-Entropy :33.219280948863776\n",
            "\n",
            "\n",
            "Language Training : czech, Language test : french\n",
            " Proba Cross-Entropy :33.21928094886473\n",
            "\n",
            "\n",
            "Language Training : czech, Language test : czech\n",
            " Proba Cross-Entropy :31.253510329428686\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "cross_entropies = {}\n",
        "\n",
        "for language_train in training_data.keys():\n",
        "  for language, test_tokens in test_data.items():\n",
        "      trigram_probs, _, _ = compute_ngram_probabilities(extract_tokens_ngrams, training_data[language_train], 3)\n",
        "      trigrams = extract_tokens_ngrams(test_tokens, 3)\n",
        "      cross_entropies[language,language_train] =compute_cross_entropy(trigrams, trigram_probs)\n",
        "\n",
        "for languages in cross_entropies.keys():\n",
        "    print(f\"Language Training : {languages[1]}, Language test : {languages[0]}\\n Proba Cross-Entropy :{cross_entropies.get(languages)}\")\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HzRKMZDb11Q"
      },
      "source": [
        "With that output, we can see that is more important when it is the good language, we will use this for the implementation of ``identify_language()``"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBaEzMCcb11Q"
      },
      "source": [
        "## Identify Language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KLfvY-Cb11Q"
      },
      "outputs": [],
      "source": [
        "def softmax(scores):\n",
        "    \"\"\"Transform scores into probabilities\"\"\"\n",
        "    exp_scores = np.exp(-np.array(list(scores.values())))  # e^(-H)\n",
        "    sum_exp_scores = np.sum(exp_scores)\n",
        "    probabilities = {lang: exp_scores[i] / sum_exp_scores for i, lang in enumerate(scores)}\n",
        "    return probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiWsc52Tb11Q"
      },
      "outputs": [],
      "source": [
        "def identify_tokens_language(text):\n",
        "    \"\"\"Identifies the language of a given text by comparing tokens trigram probabilities.\"\"\"\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokenizer = MosesTokenizer(lang='en')  # Default to English tokenizer\n",
        "    tokens_text = tokenizer.tokenize(text)\n",
        "\n",
        "    if len(tokens_text)<3 :\n",
        "        print(\"Text too short, please send at least 3 words\")\n",
        "        return None\n",
        "\n",
        "    trigrams_text = extract_tokens_ngrams(tokens_text, 3)\n",
        "\n",
        "    language_scores = {}\n",
        "\n",
        "    for language, trigram_probs in training_trigram_probs.items():\n",
        "        language_scores[language] = compute_cross_entropy(trigrams_text, trigram_probs)\n",
        "\n",
        "    language_probabilities = softmax(language_scores)\n",
        "    sorted_languages = sorted(language_probabilities.items(), key=lambda x: x[1],reverse=True)\n",
        "\n",
        "\n",
        "\n",
        "    return sorted_languages\n",
        "\n",
        "\n",
        "def identify_charac_language(text):\n",
        "    \"\"\"Identifies the language of a given text by comparing characters trigram probabilities.\"\"\"\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokenizer = MosesTokenizer(lang='en')  # Default to English tokenizer\n",
        "    tokens_text = tokenizer.tokenize(text)\n",
        "\n",
        "    if len(tokens_text) < 3 :\n",
        "        print(\"Text too short, please send at least 3 words\")\n",
        "        return None\n",
        "\n",
        "    trigrams_text = extract_charac_ngrams(tokens_text, 3)\n",
        "\n",
        "    language_scores = {}\n",
        "\n",
        "    for language, trigram_probs in training_trigram_charac_probs.items():\n",
        "        language_scores[language] = compute_cross_entropy(trigrams_text, trigram_probs)\n",
        "\n",
        "    language_probabilities = softmax(language_scores)\n",
        "    sorted_languages = sorted(language_probabilities.items(), key=lambda x: x[1],reverse=True)\n",
        "\n",
        "\n",
        "\n",
        "    return sorted_languages\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CT5pF_YQb11R"
      },
      "source": [
        "Neither of comparing using characters and tokens probabilities is very efficient, but using both reduce wrong return."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def identify_language(text):\n",
        "    \"\"\" Identifies the language of a given text by comparing trigram of characters and tokens probabilities.\"\"\"\n",
        "    tokens_prob = identify_tokens_language(text)\n",
        "    if tokens_prob[0][1] < 0.34 :\n",
        "        return identify_charac_language(text)\n",
        "    return tokens_prob"
      ],
      "metadata": {
        "id": "1C6xPcp3hUq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZuDbdF8b11R",
        "outputId": "4d752468-72c0-4491-950f-227d3d3eff50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- Test for Czech Language -----\n",
            "Test text : Jsem test a pracuji\n",
            "\n",
            "\n",
            "Both trigrams : [('czech', np.float64(0.795612974933282)), ('french', np.float64(0.18468449617678023)), ('english', np.float64(0.01970252888993776))]\n",
            "\n",
            "\n",
            "Tokens : [('english', np.float64(0.3333333333333333)), ('french', np.float64(0.3333333333333333)), ('czech', np.float64(0.3333333333333333))]\n",
            "Charac[('czech', np.float64(0.795612974933282)), ('french', np.float64(0.18468449617678023)), ('english', np.float64(0.01970252888993776))]\n",
            "\n",
            "\n",
            "----- Test for English Language -----\n",
            "Test text : I am a test and I am working\n",
            "\n",
            "\n",
            "Both trigrams : [('english', np.float64(0.983146832569954)), ('french', np.float64(0.010652516948764498)), ('czech', np.float64(0.006200650481281525))]\n",
            "\n",
            "\n",
            "Tokens : [('english', np.float64(0.3333333333333333)), ('french', np.float64(0.3333333333333333)), ('czech', np.float64(0.3333333333333333))]\n",
            "Charac[('english', np.float64(0.983146832569954)), ('french', np.float64(0.010652516948764498)), ('czech', np.float64(0.006200650481281525))]\n",
            "\n",
            "\n",
            "----- Test for French Language -----\n",
            "Test text : Je suis un test et je fonctionne\n",
            "\n",
            "\n",
            "Both trigrams : [('french', np.float64(0.5492992300760423)), ('english', np.float64(0.447876041510717)), ('czech', np.float64(0.00282472841324066))]\n",
            "\n",
            "\n",
            "Tokens : [('english', np.float64(0.3333333333333333)), ('french', np.float64(0.3333333333333333)), ('czech', np.float64(0.3333333333333333))]\n",
            "Charac[('french', np.float64(0.5492992300760423)), ('english', np.float64(0.447876041510717)), ('czech', np.float64(0.00282472841324066))]\n",
            "------------------------------------\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"----- Test for Czech Language -----\")\n",
        "text = \"Jsem test a pracuji\"\n",
        "print(f\"Test text : {text}\")\n",
        "print(\"\\n\")\n",
        "print(f\"Both trigrams : {identify_language(text)}\")\n",
        "print(\"\\n\")\n",
        "print(f\"Tokens : {identify_tokens_language(text)}\")\n",
        "print(f\"Charac{identify_charac_language(text)}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"----- Test for English Language -----\")\n",
        "text = \"I am a test and I am working\"\n",
        "print(f\"Test text : {text}\")\n",
        "print(\"\\n\")\n",
        "print(f\"Both trigrams : {identify_language(text)}\")\n",
        "print(\"\\n\")\n",
        "print(f\"Tokens : {identify_tokens_language(text)}\")\n",
        "print(f\"Charac{identify_charac_language(text)}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"----- Test for French Language -----\")\n",
        "text = \"Je suis un test et je fonctionne\"\n",
        "print(f\"Test text : {text}\")\n",
        "print(\"\\n\")\n",
        "print(f\"Both trigrams : {identify_language(text)}\")\n",
        "print(\"\\n\")\n",
        "print(f\"Tokens : {identify_tokens_language(text)}\")\n",
        "print(f\"Charac{identify_charac_language(text)}\")\n",
        "print(\"------------------------------------\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}